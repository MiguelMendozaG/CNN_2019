{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGG16.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelMendozaG/CNN_2019/blob/master/VGG16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4USVYXwEJPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23e537a8-bea5-4447-bb54-669573676aa3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2 as cv\n",
        "import time\n",
        "import numpy as np\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P03SLTaOvHFp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "720738bd-f459-4dbd-8d0d-eb884d216d17"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/datasets/', force_remount = True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/datasets/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x7hJAU4xNoi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "40f16e31-e136-486c-fb8c-97c9baea93ed"
      },
      "source": [
        "!ls \"/content/datasets/My Drive/datasets\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " data_img_1.npy   data_img_3.npy  'images_CarND Udacity'\n",
            " data_img_2.npy   data_lbl.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA-1myJ7xTjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "center = np.load('/content/datasets/My Drive/datasets/data_img_1.npy')\n",
        "left = np.load('/content/datasets/My Drive/datasets/data_img_2.npy')\n",
        "right = np.load('/content/datasets/My Drive/datasets/data_img_3.npy')\n",
        "labels = np.load('/content/datasets/My Drive/datasets/data_lbl.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t2UYkoVxkJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "left_np = np.array(left)\n",
        "center_np = np.array(center)\n",
        "right_np = np.array(right)\n",
        "labels_np = np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWqltDlbxs3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5dda56a3-83a5-4a1a-8145-fcff01127e86"
      },
      "source": [
        "print((labels_np.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8036, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqedoeALxvlm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6a4c45ec-e1af-4b4b-f2e4-fc1b448792f3"
      },
      "source": [
        "train_perc = 0.80\n",
        "#training\n",
        "training_left_img = left_np[0:int(len(left_np)*train_perc)]\n",
        "training_center_img = center_np[0:int(len(center_np)*train_perc)]\n",
        "training_right_img = right_np[0:int(len(right_np)*train_perc)]\n",
        "training_labels = labels_np[0:int(len(labels_np)*train_perc)]\n",
        "\n",
        "#testing\n",
        "testing_left_img = left_np[int(len(left_np)*train_perc):]\n",
        "testing_center_img = center_np[int(len(center_np)*train_perc):]\n",
        "testing_right_img = right_np[int(len(right_np)*train_perc):]\n",
        "testing_labels = labels_np[int(len(labels_np)*train_perc):]\n",
        "\n",
        "print(\"Size of training images array:\")\n",
        "print(\"\\t Left\", training_left_img.shape)\n",
        "print(\"\\t Center\", training_center_img.shape)\n",
        "print(\"\\t Right\", training_right_img.shape)\n",
        "print (\"Size of label array: \")\n",
        "print(\"\\t\",  training_labels.shape)\n",
        "\n",
        "print(\"\\nSize of testing images array:\")\n",
        "print(\"\\t Left\", testing_left_img.shape)\n",
        "print(\"\\t Center\", testing_center_img.shape)\n",
        "print(\"\\t Right\", testing_right_img.shape)\n",
        "print (\"Size of label array: \")\n",
        "print(\"\\t\",  testing_labels.shape)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of training images array:\n",
            "\t Left (6428, 160, 320, 3)\n",
            "\t Center (6428, 160, 320, 3)\n",
            "\t Right (6428, 160, 320, 3)\n",
            "Size of label array: \n",
            "\t (6428, 4)\n",
            "\n",
            "Size of testing images array:\n",
            "\t Left (1608, 160, 320, 3)\n",
            "\t Center (1608, 160, 320, 3)\n",
            "\t Right (1608, 160, 320, 3)\n",
            "Size of label array: \n",
            "\t (1608, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy-iHn-PhZ6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c39b035a-43af-453f-8e66-072dca42c017"
      },
      "source": [
        "total_size = 500\n",
        "\n",
        "data_training = np.ones((total_size, 224*224*3))*128\n",
        "\n",
        "random_data = np.random.randint(10, size = total_size)\n",
        "labels_training  = np.zeros((total_size, 10))\n",
        "labels_training[np.arange(total_size), random_data] = 1\n",
        "\n",
        "\n",
        "data_test = data_training[0:100]\n",
        "labels_test = labels_training[0:100] \n",
        "\n",
        "print(\"Training data\")\n",
        "print(data_training.shape)\n",
        "print(labels_training.shape)\n",
        "\n",
        "\n",
        "print(\"Test data\")\n",
        "print(data_test.shape)\n",
        "print(labels_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data\n",
            "(500, 150528)\n",
            "(500, 10)\n",
            "Test data\n",
            "(100, 150528)\n",
            "(100, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4z5RlcQENFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def VGG16(img, keep_rate = 0.8, n_classes=10, batch_size = 100):\n",
        "    #limpiar graficas anteriores\n",
        "    #reset_graph()\n",
        "\n",
        "    \n",
        "    #Imagenes \n",
        "\n",
        "    #tf.summary.image(\"Image\", img)\n",
        "    \n",
        "    img = tf.reshape(img, shape=[-1, 224,224,3])\n",
        "    \n",
        "    # Declarando las variables \n",
        "    weights = {'W_conv1':tf.Variable(tf.random_normal([3,3,3,64])),\n",
        "               'W_conv2':tf.Variable(tf.random_normal([3,3,64,64])),\n",
        "               'W_conv3':tf.Variable(tf.random_normal([3,3,64,128])),\n",
        "               'W_conv4':tf.Variable(tf.random_normal([3,3,128,128])),\n",
        "               'W_conv5':tf.Variable(tf.random_normal([3,3,128,256])),\n",
        "               'W_conv6':tf.Variable(tf.random_normal([3,3,256,256])),\n",
        "               'W_conv7':tf.Variable(tf.random_normal([3,3,256,256])),\n",
        "               'W_conv8':tf.Variable(tf.random_normal([3,3,256,512])),\n",
        "               'W_conv9':tf.Variable(tf.random_normal([3,3,512,512])),\n",
        "               'W_conv10':tf.Variable(tf.random_normal([3,3,512,512])),\n",
        "               'W_conv11':tf.Variable(tf.random_normal([3,3,512,512])),\n",
        "               'W_conv12':tf.Variable(tf.random_normal([3,3,512,512])),\n",
        "               'W_conv13':tf.Variable(tf.random_normal([3,3,512,512])),\n",
        "               'W_fc1':tf.Variable(tf.random_normal([7*7*512,4096])),\n",
        "               'W_fc2':tf.Variable(tf.random_normal([4096,4096])),\n",
        "               'out':tf.Variable(tf.random_normal([4096, n_classes]))}\n",
        "\n",
        "    biases = {'b_conv1':tf.Variable(tf.random_normal([64])),\n",
        "              'b_conv2':tf.Variable(tf.random_normal([64])),\n",
        "              'b_conv3':tf.Variable(tf.random_normal([128])),\n",
        "              'b_conv4':tf.Variable(tf.random_normal([128])),\n",
        "              'b_conv5':tf.Variable(tf.random_normal([256])),\n",
        "              'b_conv6':tf.Variable(tf.random_normal([256])),\n",
        "              'b_conv7':tf.Variable(tf.random_normal([256])),\n",
        "              'b_conv8':tf.Variable(tf.random_normal([512])),\n",
        "              'b_conv9':tf.Variable(tf.random_normal([512])),\n",
        "              'b_conv10':tf.Variable(tf.random_normal([512])),\n",
        "              'b_conv11':tf.Variable(tf.random_normal([512])),\n",
        "              'b_conv12':tf.Variable(tf.random_normal([512])),\n",
        "              'b_conv13':tf.Variable(tf.random_normal([512])),\n",
        "              'b_fc1':tf.Variable(tf.random_normal([4096])),\n",
        "              'b_fc2':tf.Variable(tf.random_normal([4096])),\n",
        "              'out':tf.Variable(tf.random_normal([n_classes]))}\n",
        "   \n",
        "    \n",
        "    \n",
        "    # Declarando la arquitectura\n",
        "    \n",
        "    #Input: 224x224x3     Output: 224x224x64\n",
        "    l1 = tf.nn.conv2d(img, weights['W_conv1'], strides=[1,1,1,1], padding='SAME')\n",
        "    l1 = tf.add(l1, biases['b_conv1'])\n",
        "    l1 = tf.nn.relu(l1)\n",
        "    print(\"l1\",l1.shape)\n",
        "    l1 = tf.nn.dropout(l1, keep_rate)\n",
        "    \n",
        "    #Input: 224x224x64     Output: 224x224x64\n",
        "    l2 = tf.nn.conv2d(l1, weights['W_conv2'], strides=[1,1,1,1], padding='SAME')\n",
        "    l2 = tf.add(l2, biases['b_conv2'])\n",
        "    l2 = tf.nn.relu(l2)    \n",
        "    \n",
        "    #Input: 224x224x64     Output: 112x112x64\n",
        "    l2 = tf.nn.max_pool(l2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    print(\"l2\",l2.shape)\n",
        "    l2 = tf.nn.dropout(l2, keep_rate)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    #Input: 112x112x64     Output: 112x112x128\n",
        "    l3 = tf.nn.conv2d(l2, weights['W_conv3'], strides=[1,1,1,1], padding='SAME')\n",
        "    l3 = tf.add(l3, biases['b_conv3'])\n",
        "    l3 = tf.nn.relu(l3)\n",
        "    print(\"l3\",l3.shape)\n",
        "    #l3 = tf.nn.dropout(l3, keep_rate)\n",
        "    \n",
        "    #Input: 112x112x128     Output: 112x112x128\n",
        "    l4 = tf.nn.conv2d(l3, weights['W_conv4'], strides=[1,1,1,1], padding='SAME')\n",
        "    l4 = tf.add(l4, biases['b_conv4'])\n",
        "    l4 = tf.nn.relu(l4)\n",
        "    print(\"l4\",l4.shape)\n",
        "    #l4 = tf.nn.dropout(l4, keep_rate)\n",
        "    \n",
        "    #Input: 112x112x128     Output: 56x56x128\n",
        "    l4 = tf.nn.max_pool(l4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    print(\"l4\",l4.shape)\n",
        "    #l4 = tf.nn.dropout(l2, keep_rate)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #Input: 56x56x128    Output: 56x56x256\n",
        "    l5 = tf.nn.conv2d(l4, weights['W_conv5'], strides=[1,1,1,1], padding='SAME')\n",
        "    l5 = tf.add(l5, biases['b_conv5'])\n",
        "    \n",
        "    #Input: 56x56x128    Output: 56x56x256\n",
        "    l6 = tf.nn.conv2d(l5, weights['W_conv6'], strides=[1,1,1,1], padding='SAME')\n",
        "    l6 = tf.add(l6, biases['b_conv6'])\n",
        "    \n",
        "    #Input: 56x56x256    Output: 56x56x256\n",
        "    l7 = tf.nn.conv2d(l6, weights['W_conv7'], strides=[1,1,1,1], padding='SAME')\n",
        "    l7 = tf.add(l7, biases['b_conv7'])\n",
        "    \n",
        "    #Input: 56x56x256    Output: 28x28x256\n",
        "    l7 = tf.nn.max_pool(l7, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    l7 = tf.nn.relu(l7)\n",
        "    #print(\"l7\",l7.shape)\n",
        "    #l7 = tf.nn.dropout(l7, keep_rate)\n",
        "    print(\"l7\",l7.shape)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #Input: 28x28x256    Output: 28x28x512\n",
        "    l8 = tf.nn.conv2d(l7, weights['W_conv8'], strides=[1,1,1,1], padding='SAME')\n",
        "    l8 = tf.add(l8, biases['b_conv8'])\n",
        "    print(\"l8\",l8.shape)\n",
        "    \n",
        "    #Input: 28x28x512    Output: 28x28x512\n",
        "    l9 = tf.nn.conv2d(l8, weights['W_conv9'], strides=[1,1,1,1], padding='SAME')\n",
        "    l9 = tf.add(l9, biases['b_conv9'])\n",
        "    \n",
        "    #Input: 28x28x512    Output: 28x28x512\n",
        "    l10 = tf.nn.conv2d(l9, weights['W_conv10'], strides=[1,1,1,1], padding='SAME')\n",
        "    l10 = tf.add(l10, biases['b_conv10'])\n",
        "    \n",
        "    #Input: 28x28x512    Output: 14x14x512\n",
        "    l10 = tf.nn.max_pool(l10, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    l10 = tf.nn.relu(l10)\n",
        "    #l10 = tf.nn.dropout(l10, keep_rate)\n",
        "    print(\"l10\",l10.shape)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    #Input: 14x14x512    Output: 14x14x512\n",
        "    l11 = tf.nn.conv2d(l10, weights['W_conv11'], strides=[1,1,1,1], padding='SAME')\n",
        "    l11 = tf.add(l11, biases['b_conv11'])\n",
        "    \n",
        "    #Input: 14x14x512    Output: 14x14x512\n",
        "    l12 = tf.nn.conv2d(l11, weights['W_conv12'], strides=[1,1,1,1], padding='SAME')\n",
        "    l12 = tf.add(l12, biases['b_conv12'])\n",
        "    \n",
        "    #Input: 14x14x512    Output: 14x14x512\n",
        "    l13 = tf.nn.conv2d(l12, weights['W_conv13'], strides=[1,1,1,1], padding='SAME')\n",
        "    l13 = tf.add(l13, biases['b_conv13'])\n",
        "    \n",
        "    #Input: 14x14x512    Output: 7x7x512\n",
        "    l13 = tf.nn.max_pool(l13, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
        "    l13 = tf.nn.relu(l13)\n",
        "    #l13 = tf.nn.dropout(l13, keep_rate)\n",
        "    print(\"l13\",l13.shape)\n",
        "    \n",
        "    \n",
        "    #Input: 7x7x512     Output: 4096\n",
        "    fc1 = tf.reshape(l13, [-1, 7*7*512])\n",
        "    fc1 = tf.nn.relu(tf.matmul(fc1, weights['W_fc1'])+biases['b_fc1'])\n",
        "    #fc1 = tf.nn.dropout(fc1, keep_rate)\n",
        "    print(\"fc1\",fc1.shape)\n",
        "                          \n",
        "    #Input: 4096     Output: 4096\n",
        "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
        "    #fc2 = tf.nn.dropout(fc2, keep_rate)\n",
        "    print(\"fc2\",fc2.shape)\n",
        "    \n",
        "    \n",
        "    #Input: 4096     Output: 10\n",
        "    output_ = tf.matmul(fc2, weights['out'])+biases['out']\n",
        "    print(output_.shape)\n",
        "    \n",
        "    return output_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgPoxGBqPggR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "5a1e1da1-de3b-4e8f-9970-937b101e995e"
      },
      "source": [
        "# Declarando las entradas y salidas\n",
        "x=tf.placeholder('float',[None,224*224*3])\n",
        "y=tf.placeholder('float')\n",
        "keep_rate = tf.placeholder(tf.float32)\n",
        "output = VGG16(x)#################################\n",
        "\n",
        "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "correct = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, 'float'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0813 01:49:33.827698 139663399561088 deprecation.py:506] From <ipython-input-3-e3cb3efc9fb4>:56: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0813 01:49:33.886458 139663399561088 deprecation.py:323] From <ipython-input-4-e5918da952eb>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "l1 (?, 224, 224, 64)\n",
            "l2 (?, 112, 112, 64)\n",
            "l3 (?, 112, 112, 128)\n",
            "l4 (?, 112, 112, 128)\n",
            "l4 (?, 56, 56, 128)\n",
            "l7 (?, 28, 28, 256)\n",
            "l8 (?, 28, 28, 512)\n",
            "l10 (?, 14, 14, 512)\n",
            "l13 (?, 7, 7, 512)\n",
            "fc1 (?, 4096)\n",
            "fc2 (?, 4096)\n",
            "(?, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfQruLfwPqlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(x_data, y_data, eval_batch_size):\n",
        "    sess = tf.get_default_session()\n",
        "    eval_i = 0\n",
        "    total_cost = 0\n",
        "    count = 0\n",
        "    for _ in range(int(len(x_data)/eval_batch_size)):\n",
        "        eval_x = x_data[eval_i:eval_i + eval_batch_size]\n",
        "        eval_y = y_data[eval_i:eval_i + eval_batch_size]\n",
        "        eval_cost = sess.run(cost, feed_dict = {x:eval_x, y:eval_y, keep_rate: 1})\n",
        "        total_cost += eval_cost\n",
        "        eval_i = eval_i + eval_batch_size\n",
        "        count = count  + 1\n",
        "    total_cost = total_cost / count \n",
        "    return total_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i100jKrrXXjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predicted_output(x_data, y_data, k_rate = 1):\n",
        "    sess = tf.get_default_session()\n",
        "    y_predicted = []\n",
        "    for i in range(len(x_data)):\n",
        "        val = sess.run(output, feed_dict={x: x_data[i:i+1], y:y_data[i:i+1], keep_rate: k_rate, learning_rate: 0.0005})\n",
        "        y_predicted.append(val)\n",
        "    return y_predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZ5TSk2GXY1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_nn(epochs = 600, batch_size = 100, save = False):\n",
        "    saver = tf.train.Saver()\n",
        "    start_time = time.time()\n",
        "    train_accuracy = []\n",
        "    test_accuracy = []\n",
        "    epoch_loss_ = []\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        #writer = tf.summary.FileWriter(\"./Octree_logs\")\n",
        "        #tf.summary.FileWriter.add_graph(writer,sess.graph)\n",
        "        #writer.add_graph(sess.graph)\n",
        "        i_prin = 0\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            i = 0\n",
        "            for _ in range(int(len(data_training)/batch_size)):\n",
        "                epoch_x = data_training[i:i+batch_size]\n",
        "                epoch_y = labels_training[i:i+batch_size]\n",
        "                #feed_dict={DNN[\"x\"]: epoch_x,DNN[\"y\"]: epoch_y}\n",
        "                sess.run(optimizer ,feed_dict={x: epoch_x, y: epoch_y, keep_rate: 0.7})\n",
        "                c = sess.run(cost, feed_dict = {x: epoch_x, y: epoch_y, keep_rate: 1})\n",
        "                epoch_loss += c\n",
        "                i =i+batch_size\n",
        "            \n",
        "            epoch_loss_.append(epoch_loss)\n",
        "            train_accuracy.append(evaluate(data_training, labels_training, batch_size))\n",
        "            test_accuracy.append(evaluate(data_test, labels_test, batch_size))\n",
        "            \n",
        "            #tf.summary.scalar('loss',epoch_loss)\n",
        "            #tf.summary.scalar(\"train_accuracy\",train_accuracy)\n",
        "            #tf.summary.scalar(\"test_accuracy\",test_accuracy)\n",
        "            #summ = tf.summary.merge_all()\n",
        "            \n",
        "            if (epoch%20 == 0):\n",
        "                pass\n",
        "                #writer.add_summary(epoch_loss, epoch).eval()\n",
        "            if (epoch%50 ==0):\n",
        "                if isinstance(save, bool):\n",
        "                    ENCname=\"./SVEoctree/\"+str(epoch)+\".ckpt\"\n",
        "                    saver.save(sess, ENCname)\n",
        "\n",
        "            print(\"Epoch: \", epoch, \"Error: \", epoch_loss_[i_prin])\n",
        "            print(\"Train accuracy \\t \\t\", train_accuracy[i_prin])\n",
        "            print(\"Test accuracy \\t \\t\", test_accuracy[i_prin])\n",
        "            i_prin = i_prin + 1\n",
        "\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    return train_accuracy, test_accuracy, epoch_loss_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ULG9mEXXaTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b9ac4537-8fff-47a0-cd1e-fea9bf19ca69"
      },
      "source": [
        "train_accuracy, test_accuracy, epoch_loss = train_nn(4, 100, False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0 Error:  nan\n",
            "Train accuracy \t \t nan\n",
            "Test accuracy \t \t nan\n",
            "Epoch:  1 Error:  nan\n",
            "Train accuracy \t \t nan\n",
            "Test accuracy \t \t nan\n",
            "Epoch:  2 Error:  nan\n",
            "Train accuracy \t \t nan\n",
            "Test accuracy \t \t nan\n",
            "Epoch:  3 Error:  nan\n",
            "Train accuracy \t \t nan\n",
            "Test accuracy \t \t nan\n",
            "--- 86.70412540435791 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g-Ec6v_jePr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}